{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gymnasium - An API standard for reinforcement learning with a diverse collection of reference environments\n",
        "\n",
        "Documents: https://gymnasium.farama.org/introduction/train_agent/\n",
        "\n",
        "Gymnasium is a project that provides an API (application programming interface) for all single agent reinforcement learning environments, with implementations of common environments: cartpole, pendulum, mountain-car, mujoco, atari, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 - Training, Saving and Loading\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "Examples with Collab Code: [https://stable-baselines3.readthedocs.io/en/master/guide/examples.html](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Open a Terminal and install the following packages:\n",
        "\n",
        "sudo apt-get install build-essential python-dev-is-python3 swig python3-pygame git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. autoformatting and install box2d-py and stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3gGbS4lII8i_"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWskDE2c9WoN",
        "outputId": "bdd6e935-52e2-4fa3-f64c-cd78d6447468"
      },
      "outputs": [],
      "source": [
        "# Use pip to install box2d-py and stable-baselines3[extra] which required >= 2.0.0a4\n",
        "# and gymnasium[other] which includes pymovie\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, and create directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "# import libaries: gymnasium, numpy, stable_baselines3 and stable_baselines3.common.callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories for models, videos and tb_logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use Lunar Lander environment.\n",
        "\n",
        "\"Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine. \"\n",
        "\n",
        "Lunar Lander environment: [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "![Lunar Lander](https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif)\n",
        "\n",
        "\n",
        "We chose the MlpPolicy because input of Lunar Lander is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "# Create env for evaluation and record a video\n",
        "#   Save video progress in directory: video_proress_dir\n",
        "#   Save the first video for the first episode\n",
        "\n",
        "# Create a DQN model for evaluation using the following parameters:\n",
        "# - policy: \"MlpPolicy\"\n",
        "# - environment: env\n",
        "# - verbose: 1\n",
        "# - exploration_final_eps: 0.1\n",
        "# - target_update_interval: 250\n",
        "# - tensorboard_log: log_dir \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "We load a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeaVBGuJwK97"
      },
      "outputs": [],
      "source": [
        "# import a helper function: evaluate_policy to evaluate the policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# Before training, how agent is performed and its mean of rewards and std of rewards\n",
        "\n",
        "# print out its mean of rewards and std of rewards and video saved in video_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and save it\n",
        "\n",
        "1. Create an env \n",
        "2. Record video \n",
        "3. Create DQN model\n",
        "4. Record the result very 10000 steps\n",
        "5. RL 100,000 (1e5)\n",
        "6. Save the final result\n",
        "\n",
        "Warning: this may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "# Trigger video creation every 50 episodes\n",
        "\n",
        "# Create an env and Record video\n",
        "\n",
        "# Create a DQN model\n",
        "\n",
        "# Save every 10000 steps, save the result under \"models/dqn_lunar\" with prefix \"dqn_lunar\" using CheckpointCallback\n",
        "\n",
        "# Start training the agent with timesteps of 100000, using checkpoint callback and setting tensorboard log name to \"dqn_lunar\"\n",
        "\n",
        "# Save the final agent result optionally\n",
        "\n",
        "# delete trained model to demonstrate loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T31dZJYNrJwF"
      },
      "source": [
        "## Load the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ExgtyZrIA6"
      },
      "outputs": [],
      "source": [
        "# load the final checkpoint file dqn_lunar_final.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation env and create a video using RecordVideo wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "\n",
        "# print out the mean of rewards and std of rewards after training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# close the environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ==============================================\n",
        "\n",
        "# Final Task: If time is allowed, increase the training frequency such that the mean_reward score can go up to 100+ mark. Can you tell how many times you need to train the model ?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
